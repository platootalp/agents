{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d92c63",
   "metadata": {},
   "source": [
    "# 异步RAG生成器实现示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d7d41",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63738ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "from typing import List, AsyncGenerator, Optional\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "\n",
    "# 设置日志\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3d4c08",
   "metadata": {},
   "source": [
    "## 2. 异步文档加载与分块\n",
    "\n",
    "本节实现异步文档加载器和智能分块器，支持高并发文件处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da70f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    id: str\n",
    "    content: str\n",
    "    metadata: dict\n",
    "\n",
    "class AsyncDocumentLoader:\n",
    "    \"\"\"异步文档加载器\"\"\"\n",
    "    def __init__(self, max_concurrent=10):\n",
    "        self.semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    \n",
    "    async def load_file(self, filepath: str) -> Document:\n",
    "        \"\"\"异步加载本地文件\"\"\"\n",
    "        async with self.semaphore:\n",
    "            # 模拟异步文件读取（实际使用aiofiles）\n",
    "            await asyncio.sleep(0.1)  # 模拟IO延迟\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            return Document(\n",
    "                id=f\"doc_{hash(filepath)}\",\n",
    "                content=content,\n",
    "                metadata={\"source\": filepath, \"size\": len(content)}\n",
    "            )\n",
    "    \n",
    "    async def load_batch(self, filepaths: List[str]) -> List[Document]:\n",
    "        \"\"\"批量异步加载文档\"\"\"\n",
    "        tasks = [self.load_file(fp) for fp in filepaths]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "class AsyncChunker:\n",
    "    \"\"\"异步文本分块器\"\"\"\n",
    "    def __init__(self, chunk_size=500, overlap=50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "    \n",
    "    async def chunk_document(self, doc: Document) -> List[Document]:\n",
    "        \"\"\"将文档异步分块\"\"\"\n",
    "        # 模拟异步分块处理\n",
    "        await asyncio.sleep(0.05)\n",
    "        content = doc.content\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        while start < len(content):\n",
    "            end = min(start + self.chunk_size, len(content))\n",
    "            chunk_content = content[start:end]\n",
    "            chunks.append(Document(\n",
    "                id=f\"{doc.id}_chunk_{chunk_id}\",\n",
    "                content=chunk_content,\n",
    "                metadata={\n",
    "                    **doc.metadata,\n",
    "                    \"chunk_index\": chunk_id,\n",
    "                    \"start_pos\": start,\n",
    "                    \"end_pos\": end\n",
    "                }\n",
    "            ))\n",
    "            start += self.chunk_size - self.overlap\n",
    "            chunk_id += 1\n",
    "        \n",
    "        logger.info(f\"文档 {doc.id} 分块为 {len(chunks)} 个片段\")\n",
    "        return chunks\n",
    "\n",
    "# 示例用法\n",
    "async def demo_loader_chunker():\n",
    "    loader = AsyncDocumentLoader()\n",
    "    chunker = AsyncChunker()\n",
    "    \n",
    "    # 模拟文件路径\n",
    "    filepaths = [\"data/doc1.txt\", \"data/doc2.txt\"]\n",
    "    \n",
    "    # 异步加载文档\n",
    "    docs = await loader.load_batch(filepaths)\n",
    "    print(f\"加载了 {len(docs)} 个文档\")\n",
    "    \n",
    "    # 异步分块\n",
    "    all_chunks = []\n",
    "    for doc in docs:\n",
    "        chunks = await chunker.chunk_document(doc)\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    print(f\"总块数: {len(all_chunks)}\")\n",
    "    return all_chunks\n",
    "\n",
    "# 运行示例（在Jupyter中直接运行）\n",
    "await demo_loader_chunker()  # 注意：在普通Python中需要asyncio.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e152926c",
   "metadata": {},
   "source": [
    "## 3. 向量检索的异步封装\n",
    "\n",
    "本节实现异步向量嵌入生成和检索，包含连接池与批处理优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c652ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncEmbedder:\n",
    "    \"\"\"异步向量嵌入生成器\"\"\"\n",
    "    def __init__(self, model_name=\"text-embedding-ada-002\", batch_size=32):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.cache = {}  # 简单缓存\n",
    "    \n",
    "    async def embed_text(self, text: str) -> List[float]:\n",
    "        \"\"\"异步生成单个文本的嵌入向量\"\"\"\n",
    "        if text in self.cache:\n",
    "            return self.cache[text]\n",
    "        \n",
    "        # 模拟异步API调用（实际使用OpenAI异步客户端）\n",
    "        await asyncio.sleep(0.2)\n",
    "        # 生成随机向量（模拟）\n",
    "        embedding = list(np.random.randn(1536))\n",
    "        self.cache[text] = embedding\n",
    "        return embedding\n",
    "    \n",
    "    async def embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"批量异步嵌入，提升吞吐\"\"\"\n",
    "        tasks = [self.embed_text(text) for text in texts]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "class AsyncVectorStore:\n",
    "    \"\"\"异步向量存储与检索器\"\"\"\n",
    "    def __init__(self, embedder: AsyncEmbedder, max_connections=5):\n",
    "        self.embedder = embedder\n",
    "        self.semaphore = asyncio.Semaphore(max_connections)\n",
    "        # 模拟向量存储（实际连接Qdrant/Pinecone）\n",
    "        self.vectors = {}\n",
    "        self.metadata = {}\n",
    "    \n",
    "    async def add_documents(self, documents: List[Document]):\n",
    "        \"\"\"异步添加文档到向量存储\"\"\"\n",
    "        texts = [doc.content for doc in documents]\n",
    "        embeddings = await self.embedder.embed_batch(texts)\n",
    "        \n",
    "        for doc, emb in zip(documents, embeddings):\n",
    "            self.vectors[doc.id] = emb\n",
    "            self.metadata[doc.id] = doc.metadata\n",
    "    \n",
    "    async def search(self, query: str, top_k=5) -> List[Document]:\n",
    "        \"\"\"异步相似度检索\"\"\"\n",
    "        async with self.semaphore:\n",
    "            query_embedding = await self.embedder.embed_text(query)\n",
    "            \n",
    "            # 计算余弦相似度（模拟）\n",
    "            results = []\n",
    "            for doc_id, doc_embedding in self.vectors.items():\n",
    "                # 简化相似度计算\n",
    "                similarity = np.dot(query_embedding, doc_embedding) / (\n",
    "                    np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)\n",
    "                )\n",
    "                results.append((similarity, doc_id))\n",
    "            \n",
    "            # 按相似度排序\n",
    "            results.sort(reverse=True)\n",
    "            \n",
    "            retrieved_docs = []\n",
    "            for similarity, doc_id in results[:top_k]:\n",
    "                doc = Document(\n",
    "                    id=doc_id,\n",
    "                    content=f\"[相似度: {similarity:.3f}] {self.metadata[doc_id].get('source', 'unknown')}\",\n",
    "                    metadata={**self.metadata[doc_id], \"similarity\": similarity}\n",
    "                )\n",
    "                retrieved_docs.append(doc)\n",
    "            \n",
    "            return retrieved_docs\n",
    "\n",
    "# 示例：构建和检索\n",
    "async def demo_retrieval():\n",
    "    # 创建组件\n",
    "    embedder = AsyncEmbedder()\n",
    "    vector_store = AsyncVectorStore(embedder)\n",
    "    \n",
    "    # 模拟一些文档块\n",
    "    mock_docs = [\n",
    "        Document(id=\"doc1\", content=\"Python异步编程使用asyncio库\", metadata={\"type\": \"technical\"}),\n",
    "        Document(id=\"doc2\", content=\"RAG系统结合检索与生成技术\", metadata={\"type\": \"overview\"}),\n",
    "        Document(id=\"doc3\", content=\"向量数据库如Qdrant支持高维相似度搜索\", metadata={\"type\": \"database\"}),\n",
    "    ]\n",
    "    \n",
    "    await vector_store.add_documents(mock_docs)\n",
    "    \n",
    "    # 异步检索\n",
    "    query = \"如何实现异步RAG系统？\"\n",
    "    results = await vector_store.search(query, top_k=2)\n",
    "    \n",
    "    print(\"检索结果:\")\n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"{i+1}. {doc.content}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fec752",
   "metadata": {},
   "source": [
    "## 4. 基于生成器的流式响应生成\n",
    "\n",
    "本节实现异步生成器，逐token流式返回LLM响应，支持实时交互。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b5b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncStreamingGenerator:\n",
    "    \"\"\"异步流式生成器\"\"\"\n",
    "    def __init__(self, chunk_delay=0.1):\n",
    "        self.chunk_delay = chunk_delay\n",
    "    \n",
    "    async def generate_stream(self, context: str, query: str) -> AsyncGenerator[str, None]:\n",
    "        \"\"\"异步生成器，逐块返回响应\"\"\"\n",
    "        # 模拟LLM流式生成（实际使用OpenAI异步流式API）\n",
    "        prompt = f\"基于以下上下文回答用户问题：\n",
    "                    上下文：{context}\n",
    "                    问题：{query}\n",
    "                    回答：\n",
    "                \"\n",
    "        \n",
    "        # 模拟分块响应\n",
    "        response_parts = [\n",
    "            \"基于您的问题和提供的上下文，\",\n",
    "            \"异步RAG系统的实现需要几个关键组件：\",\n",
    "            \"1. 异步文档加载与分块\",\n",
    "            \"2. 向量检索的异步封装\",\n",
    "            \"3. 基于生成器的流式响应生成\",\n",
    "            \"4. 并发控制与超时处理。\",\n",
    "            \"这些组件共同构建了一个高并发、低延迟的检索增强生成系统。\"\n",
    "        ]\n",
    "        \n",
    "        for part in response_parts:\n",
    "            await asyncio.sleep(self.chunk_delay)  # 模拟生成延迟\n",
    "            yield part\n",
    "    \n",
    "    async def generate_full_response(self, context: str, query: str) -> str:\n",
    "        \"\"\"异步生成完整响应（非流式）\"\"\"\n",
    "        full_response = \"\"\n",
    "        async for chunk in self.generate_stream(context, query):\n",
    "            full_response += chunk + \" \"\n",
    "        return full_response.strip()\n",
    "\n",
    "# 示例：流式生成\n",
    "async def demo_streaming():\n",
    "    generator = AsyncStreamingGenerator(chunk_delay=0.05)\n",
    "    \n",
    "    context = \"异步RAG系统使用asyncio实现高并发，结合向量数据库进行相似度检索。\"\n",
    "    query = \"请解释异步RAG系统的优势。\"\n",
    "    \n",
    "    print(\"开始流式生成...\")\n",
    "    async for chunk in generator.generate_stream(context, query):\n",
    "        print(f\"收到块: {chunk}\")\n",
    "    \n",
    "    print(\"\\n流式生成完成！\")\n",
    "\n",
    "# 示例：完整流程集成\n",
    "async def demo_full_rag():\n",
    "    \"\"\"完整异步RAG流程演示\"\"\"\n",
    "    # 初始化组件\n",
    "    loader = AsyncDocumentLoader()\n",
    "    chunker = AsyncChunker()\n",
    "    embedder = AsyncEmbedder()\n",
    "    vector_store = AsyncVectorStore(embedder)\n",
    "    generator = AsyncStreamingGenerator()\n",
    "    \n",
    "    # 1. 加载和分块文档（模拟）\n",
    "    mock_doc = Document(\n",
    "        id=\"tech_doc\",\n",
    "        content=\"\"\"异步编程允许程序在等待IO操作时执行其他任务。\n",
    "        Python的asyncio提供了事件循环和协程支持。\n",
    "        RAG系统结合检索与生成，提升大模型回答的准确性和时效性。\"\"\",\n",
    "        metadata={\"source\": \"tech_guide.txt\"}\n",
    "    )\n",
    "    chunks = await chunker.chunk_document(mock_doc)\n",
    "    await vector_store.add_documents(chunks)\n",
    "    \n",
    "    # 2. 用户查询\n",
    "    query = \"Python中如何实现异步编程？\"\n",
    "    \n",
    "    # 3. 异步检索\n",
    "    retrieved = await vector_store.search(query, top_k=2)\n",
    "    context = \"\\n\".join([doc.content for doc in retrieved])\n",
    "    \n",
    "    # 4. 流式生成\n",
    "    print(\"RAG系统回答:\")\n",
    "    async for chunk in generator.generate_stream(context, query):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\\n--- 流程结束 ---\")\n",
    "\n",
    "# 注意：在Jupyter中可以直接运行await demo_full_rag()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52db905",
   "metadata": {},
   "source": [
    "## 5. 并发控制与超时处理\n",
    "\n",
    "本节展示如何使用asyncio原语实现并发控制、超时和错误恢复。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad422dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncRAGOrchestrator:\n",
    "    \"\"\"异步RAG编排器，负责并发控制与错误处理\"\"\"\n",
    "    def __init__(self, max_concurrent_requests=10, timeout_seconds=30):\n",
    "        self.semaphore = asyncio.Semaphore(max_concurrent_requests)\n",
    "        self.timeout = timeout_seconds\n",
    "        self.request_count = 0\n",
    "    \n",
    "    async def process_query_with_timeout(self, query: str) -> Optional[str]:\n",
    "        \"\"\"带超时和并发限制的查询处理\"\"\"\n",
    "        async with self.semaphore:\n",
    "            self.request_count += 1\n",
    "            req_id = self.request_count\n",
    "            \n",
    "            try:\n",
    "                # 设置整体超时\n",
    "                result = await asyncio.wait_for(\n",
    "                    self._process_query(query, req_id),\n",
    "                    timeout=self.timeout\n",
    "                )\n",
    "                logger.info(f\"请求 {req_id} 成功完成\")\n",
    "                return result\n",
    "            except asyncio.TimeoutError:\n",
    "                logger.error(f\"请求 {req_id} 超时（{self.timeout}秒）\")\n",
    "                return \"请求超时，请稍后重试\"\n",
    "            except Exception as e:\n",
    "                logger.error(f\"请求 {req_id} 出错: {e}\")\n",
    "                return f\"处理出错: {str(e)}\"\n",
    "    \n",
    "    async def _process_query(self, query: str, req_id: int) -> str:\n",
    "        \"\"\"模拟处理逻辑\"\"\"\n",
    "        logger.info(f\"开始处理请求 {req_id}: {query[:50]}...\")\n",
    "        await asyncio.sleep(0.5)  # 模拟处理时间\n",
    "        return f\"对查询'{query}'的模拟响应\"\n",
    "    \n",
    "    async def batch_process(self, queries: List[str]) -> List[str]:\n",
    "        \"\"\"批量处理多个查询，限制并发数\"\"\"\n",
    "        tasks = [self.process_query_with_timeout(q) for q in queries]\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        # 处理异常结果\n",
    "        final_results = []\n",
    "        for i, res in enumerate(results):\n",
    "            if isinstance(res, Exception):\n",
    "                final_results.append(f\"查询'{queries[i]}'失败: {res}\")\n",
    "            else:\n",
    "                final_results.append(res)\n",
    "        \n",
    "        return final_results\n",
    "\n",
    "# 示例：并发控制演示\n",
    "async def demo_concurrency():\n",
    "    orchestrator = AsyncRAGOrchestrator(max_concurrent_requests=3, timeout_seconds=2)\n",
    "    \n",
    "    # 模拟一批查询\n",
    "    queries = [f\"问题{i}\" for i in range(10)]\n",
    "    \n",
    "    print(f\"开始批量处理 {len(queries)} 个查询（最大并发3）...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    results = await orchestrator.batch_process(queries)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    print(f\"批量处理完成，耗时 {elapsed:.2f} 秒\")\n",
    "    print(f\"成功结果数: {len([r for r in results if '失败' not in r])}\")\n",
    "    \n",
    "    # 显示部分结果\n",
    "    for i, res in enumerate(results[:3]):\n",
    "        print(f\"结果{i+1}: {res[:50]}...\")\n",
    "\n",
    "# 超时处理示例\n",
    "async def demo_timeout():\n",
    "    \"\"\"演示超时处理\"\"\"\n",
    "    orchestrator = AsyncRAGOrchestrator(max_concurrent_requests=1, timeout_seconds=1)\n",
    "    \n",
    "    # 这个查询会超时，因为模拟处理需要2秒\n",
    "    async def slow_query():\n",
    "        await asyncio.sleep(2)\n",
    "        return \"应该超时\"\n",
    "    \n",
    "    result = await orchestrator.process_query_with_timeout(\"慢查询\")\n",
    "    print(f\"超时处理结果: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3564b100",
   "metadata": {},
   "source": [
    "## 6. 单元测试示例\n",
    "\n",
    "本节提供异步单元测试示例，确保组件可靠性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3463c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_async_document_loader():\n",
    "    \"\"\"测试异步文档加载器\"\"\"\n",
    "    loader = AsyncDocumentLoader()\n",
    "    \n",
    "    # 创建临时测试文件\n",
    "    test_content = \"测试文档内容\"\n",
    "    with open(\"test_doc.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(test_content)\n",
    "    \n",
    "    try:\n",
    "        doc = await loader.load_file(\"test_doc.txt\")\n",
    "        assert doc.id.startswith(\"doc_\")\n",
    "        assert doc.content == test_content\n",
    "        assert \"source\" in doc.metadata\n",
    "        print(\"✓ AsyncDocumentLoader 测试通过\")\n",
    "    finally:\n",
    "        import os\n",
    "        os.remove(\"test_doc.txt\")\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_async_chunker():\n",
    "    \"\"\"测试异步分块器\"\"\"\n",
    "    chunker = AsyncChunker(chunk_size=100, overlap=20)\n",
    "    doc = Document(id=\"test\", content=\"a\" * 300, metadata={})\n",
    "    \n",
    "    chunks = await chunker.chunk_document(doc)\n",
    "    \n",
    "    assert len(chunks) > 1\n",
    "    total_content = \"\".join([c.content for c in chunks])\n",
    "    assert total_content == doc.content\n",
    "    \n",
    "    # 检查重叠\n",
    "    chunk_contents = [c.content for c in chunks]\n",
    "    for i in range(len(chunk_contents)-1):\n",
    "        overlap_region = chunk_contents[i][-20:] if len(chunk_contents[i]) >= 20 else chunk_contents[i]\n",
    "        next_start = chunk_contents[i+1][:20] if len(chunk_contents[i+1]) >= 20 else chunk_contents[i+1]\n",
    "        # 实际实现中应有重叠检查，这里简化\n",
    "        pass\n",
    "    \n",
    "    print(\"✓ AsyncChunker 测试通过\")\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_async_vector_store():\n",
    "    \"\"\"测试异步向量存储\"\"\"\n",
    "    embedder = AsyncEmbedder()\n",
    "    vector_store = AsyncVectorStore(embedder)\n",
    "    \n",
    "    docs = [\n",
    "        Document(id=\"doc1\", content=\"机器学习\", metadata={}),\n",
    "        Document(id=\"doc2\", content=\"深度学习\", metadata={}),\n",
    "    ]\n",
    "    \n",
    "    await vector_store.add_documents(docs)\n",
    "    \n",
    "    results = await vector_store.search(\"学习\", top_k=2)\n",
    "    assert len(results) == 2\n",
    "    assert all(isinstance(doc, Document) for doc in results)\n",
    "    \n",
    "    print(\"✓ AsyncVectorStore 测试通过\")\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_streaming_generator():\n",
    "    \"\"\"测试流式生成器\"\"\"\n",
    "    generator = AsyncStreamingGenerator(chunk_delay=0.01)\n",
    "    \n",
    "    chunks_collected = []\n",
    "    async for chunk in generator.generate_stream(\"上下文\", \"问题\"):\n",
    "        chunks_collected.append(chunk)\n",
    "    \n",
    "    assert len(chunks_collected) > 0\n",
    "    assert \"回答\" in \"\".join(chunks_collected)\n",
    "    \n",
    "    print(\"✓ AsyncStreamingGenerator 测试通过\")\n",
    "\n",
    "# 运行测试的函数\n",
    "async def run_all_tests():\n",
    "    \"\"\"运行所有测试\"\"\"\n",
    "    print(\"开始运行异步RAG组件测试...\")\n",
    "    \n",
    "    await test_async_document_loader()\n",
    "    await test_async_chunker()\n",
    "    await test_async_vector_store()\n",
    "    await test_streaming_generator()\n",
    "    \n",
    "    print(\"\\n所有测试通过！\")\n",
    "\n",
    "# 在Jupyter中可以直接运行：\n",
    "# await run_all_tests()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f529a7",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本笔记本实现了一个完整的异步RAG生成器原型，包含：\n",
    "\n",
    "1. **异步文档加载与分块**：支持高并发文件处理\n",
    "2. **向量检索的异步封装**：包含嵌入生成和相似度检索\n",
    "3. **基于生成器的流式响应生成**：逐token返回LLM响应\n",
    "4. **并发控制与超时处理**：使用信号量和超时原语\n",
    "5. **单元测试示例**：确保组件可靠性\n",
    "\n",
    "**扩展建议**：\n",
    "- 替换模拟组件为真实服务（OpenAI API、Qdrant等）\n",
    "- 添加更复杂的重排序算法（交叉编码器）\n",
    "- 集成监控和指标收集（Prometheus）\n",
    "- 实现Skill化架构，支持动态加载检索/生成模块\n",
    "\n",
    "此实现为企业级AI Agent系统提供了可扩展的异步RAG基础框架。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
