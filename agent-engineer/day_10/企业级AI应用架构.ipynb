{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä¼ä¸šçº§AIåº”ç”¨æ¶æ„å®ç°\n",
    "\n",
    "## æ¦‚è¿°\n",
    "æœ¬Notebookå±•ç¤ºäº†Day10å®è·µå·©å›ºé¡¹ç›®ï¼šä¼ä¸šçº§AIåº”ç”¨æ¶æ„å®ç°ã€‚åŒ…å«ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ï¼š\n",
    "\n",
    "1. **RAGæœåŠ¡** - å¤šçº§ç¼“å­˜ç­–ç•¥ã€æ··åˆæ£€ç´¢ä¼˜åŒ–ã€é‡æ’åºæ¨¡å‹\n",
    "2. **ç¼“å­˜æœåŠ¡** - ç¼“å­˜ç©¿é€/é›ªå´©é˜²æŠ¤ã€è¿æ¥æ± ä¼˜åŒ–ã€åºåˆ—åŒ–æ€§èƒ½\n",
    "3. **APIç½‘å…³** - åŠ¨æ€è·¯ç”±ã€é™æµç†”æ–­ã€è®¤è¯é‰´æƒã€ç»“æ„åŒ–æ—¥å¿—\n",
    "4. **ç›‘æ§ä½“ç³»** - PrometheusæŒ‡æ ‡ã€Grafanaé¢æ¿ã€ELKæ—¥å¿—ã€å‘Šè­¦è§„åˆ™\n",
    "\n",
    "## Javaç»éªŒè¿ç§»\n",
    "- ç¼“å­˜ç­–ç•¥ï¼ˆCaffeine/Redisï¼‰â†’ å¤šçº§ç¼“å­˜æ¶æ„\n",
    "- APIç½‘å…³ï¼ˆSpring Cloud Gatewayï¼‰â†’ ç»Ÿä¸€è·¯ç”±ä¸é™æµ\n",
    "- ç›‘æ§ä½“ç³»ï¼ˆMicrometer + Prometheusï¼‰â†’ æŒ‡æ ‡æ”¶é›†ä¸å±•ç¤º\n",
    "- æ—¥å¿—æ¡†æ¶ï¼ˆLogback + ELKï¼‰â†’ ç»“æ„åŒ–æ—¥å¿—èšåˆ\n",
    "\n",
    "## éªŒæ”¶æ ‡å‡†\n",
    "1. RAGæœåŠ¡å¿…é¡»æ”¯æŒå¤šçº§ç¼“å­˜ï¼Œæ£€ç´¢å‡†ç¡®ç‡ > 85%\n",
    "2. ç¼“å­˜æœåŠ¡å¿…é¡»å®ç°ç©¿é€/é›ªå´©é˜²æŠ¤ï¼Œç¼“å­˜å‘½ä¸­ç‡ > 70%\n",
    "3. APIç½‘å…³å¿…é¡»å®ç°åŠ¨æ€è·¯ç”±å’Œé™æµç†”æ–­ï¼Œè¯·æ±‚æˆåŠŸç‡ > 99%\n",
    "4. ç›‘æ§ä½“ç³»å¿…é¡»æ”¶é›†å…³é”®æŒ‡æ ‡ï¼ˆQPSã€å»¶è¿Ÿã€é”™è¯¯ç‡ï¼‰ï¼Œå¯è§†åŒ–é¢æ¿å®Œæ•´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒå‡†å¤‡ä¸ä¾èµ–å®‰è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# å®‰è£…ä¾èµ–\n",
    "%pip install fastapi uvicorn redis pymilvus openai sentence-transformers rank-bm25\n",
    "%pip install prometheus-client pydantic python-jose pybloom-live msgpack\n",
    "%pip install numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "\n",
    "# æ£€æŸ¥å…³é”®ä¾èµ–\n",
    "try:\n",
    "    import redis.asyncio as redis\n",
    "    from pymilvus import MilvusClient\n",
    "    from sentence_transformers import CrossEncoder\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    from openai import AsyncOpenAI\n",
    "    from pybloom_live import BloomFilter\n",
    "    import msgpack\n",
    "    print(\"âœ… æ‰€æœ‰ä¾èµ–åº“å¯¼å…¥æˆåŠŸ\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ ä¾èµ–åº“å¯¼å…¥å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RAGæœåŠ¡å®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLevelCacheRAG:\n",
    "    \"\"\"å¤šçº§ç¼“å­˜RAGæœåŠ¡å®ç°\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # å†…å­˜ç¼“å­˜ï¼ˆLRUï¼Œæœ€å¤§100æ¡ï¼‰\n",
    "        self.memory_cache = {}\n",
    "        self.cache_size = 100\n",
    "        \n",
    "        # ç»Ÿè®¡æŒ‡æ ‡\n",
    "        self.metrics = {\n",
    "            \"memory_cache_hits\": 0,\n",
    "            \"redis_cache_hits\": 0,\n",
    "            \"vector_searches\": 0,\n",
    "            \"average_latency\": 0.0\n",
    "        }\n",
    "        \n",
    "        # æ¨¡æ‹Ÿæ•°æ®\n",
    "        self.documents = [\n",
    "            \"AI Agentæ˜¯èƒ½å¤Ÿæ„ŸçŸ¥ç¯å¢ƒã€åšå‡ºå†³ç­–å¹¶æ‰§è¡ŒåŠ¨ä½œçš„æ™ºèƒ½ç³»ç»Ÿã€‚\",\n",
    "            \"RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰é€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†åº“æ¥å¢å¼ºLLMç”Ÿæˆè´¨é‡ã€‚\",\n",
    "            \"å‘é‡æ•°æ®åº“ä¸“é—¨ç”¨äºå­˜å‚¨å’Œæ£€ç´¢é«˜ç»´å‘é‡ï¼Œæ”¯æŒç›¸ä¼¼åº¦æœç´¢ã€‚\",\n",
    "            \"å¤šçº§ç¼“å­˜ç­–ç•¥åŒ…æ‹¬å†…å­˜ç¼“å­˜ã€åˆ†å¸ƒå¼ç¼“å­˜å’ŒæŒä¹…åŒ–å­˜å‚¨ã€‚\",\n",
    "            \"APIç½‘å…³è´Ÿè´£è·¯ç”±è½¬å‘ã€é™æµç†”æ–­ã€è®¤è¯é‰´æƒå’Œæ—¥å¿—è®°å½•ã€‚\"\n",
    "        ]\n",
    "        \n",
    "        # æ„å»ºBM25ç´¢å¼•\n",
    "        tokenized_docs = [doc.split() for doc in self.documents]\n",
    "        self.bm25_index = BM25Okapi(tokenized_docs)\n",
    "        \n",
    "        # æ¨¡æ‹Ÿé‡æ’åºæ¨¡å‹\n",
    "        self.rerank_model = lambda query, results: sorted(results, key=lambda x: random.random(), reverse=True)\n",
    "    \n",
    "    def _generate_cache_key(self, question: str) -> str:\n",
    "        \"\"\"ç”Ÿæˆç¼“å­˜é”®\"\"\"\n",
    "        return f\"rag:{hashlib.md5(question.encode()).hexdigest()}\"\n",
    "    \n",
    "    def _embed_text(self, text: str) -> List[float]:\n",
    "        \"\"\"æ¨¡æ‹Ÿæ–‡æœ¬åµŒå…¥å‘é‡ç”Ÿæˆ\"\"\"\n",
    "        np.random.seed(hash(text) % 2**32)\n",
    "        return list(np.random.randn(1536))\n",
    "    \n",
    "    def _hybrid_search(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"æ··åˆæ£€ç´¢ï¼šå‘é‡ç›¸ä¼¼åº¦ + å…³é”®è¯BM25\"\"\"\n",
    "        # æ¨¡æ‹Ÿå‘é‡æœç´¢\n",
    "        vector_scores = np.random.rand(len(self.documents))\n",
    "        \n",
    "        # BM25å…³é”®è¯æ£€ç´¢\n",
    "        tokenized_query = query.split()\n",
    "        bm25_scores = self.bm25_index.get_scores(tokenized_query)\n",
    "        \n",
    "        # å½’ä¸€åŒ–BM25åˆ†æ•°\n",
    "        if len(bm25_scores) > 0:\n",
    "            normalized_bm25 = bm25_scores / max(bm25_scores)\n",
    "        else:\n",
    "            normalized_bm25 = np.zeros(len(self.documents))\n",
    "        \n",
    "        # åŠ æƒèåˆï¼ˆå‘é‡70%ï¼Œå…³é”®è¯30%ï¼‰\n",
    "        combined_scores = 0.7 * vector_scores + 0.3 * normalized_bm25\n",
    "        \n",
    "        # ç”Ÿæˆç»“æœ\n",
    "        results = []\n",
    "        indices = np.argsort(combined_scores)[-top_k:][::-1]\n",
    "        \n",
    "        for idx in indices:\n",
    "            results.append({\n",
    "                \"content\": self.documents[idx],\n",
    "                \"confidence\": round(combined_scores[idx], 4),\n",
    "                \"source\": \"example_doc\",\n",
    "                \"page\": 1\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    async def query(self, question: str, use_cache: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"æ‰§è¡ŒRAGæŸ¥è¯¢\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # ç”Ÿæˆç¼“å­˜é”®\n",
    "        cache_key = self._generate_cache_key(question)\n",
    "        \n",
    "        # 1. æ£€æŸ¥å†…å­˜ç¼“å­˜\n",
    "        if use_cache and cache_key in self.memory_cache:\n",
    "            latency = time.time() - start_time\n",
    "            self._update_metrics(latency, source=\"memory\")\n",
    "            self.metrics[\"memory_cache_hits\"] += 1\n",
    "            return {\n",
    "                \"answer\": \"ä»å†…å­˜ç¼“å­˜è·å–çš„ç­”æ¡ˆ\",\n",
    "                \"sources\": self.memory_cache[cache_key],\n",
    "                \"cache_source\": \"memory\",\n",
    "                \"latency\": latency\n",
    "            }\n",
    "        \n",
    "        # 2. æ¨¡æ‹ŸRedisç¼“å­˜ï¼ˆç®€åŒ–ï¼‰\n",
    "        if use_cache and random.random() < 0.4:  # 40%å‘½ä¸­ç‡\n",
    "            latency = time.time() - start_time\n",
    "            self._update_metrics(latency, source=\"redis\")\n",
    "            self.metrics[\"redis_cache_hits\"] += 1\n",
    "            \n",
    "            # æ¨¡æ‹ŸRedisæ•°æ®\n",
    "            result_data = {\n",
    "                \"answer\": f\"åŸºäºæ£€ç´¢ï¼Œ{question}çš„ç­”æ¡ˆæ˜¯...\",\n",
    "                \"sources\": [{\n",
    "                    \"content\": \"ç¤ºä¾‹ç¼“å­˜å†…å®¹\",\n",
    "                    \"confidence\": 0.92,\n",
    "                    \"source\": \"cached_doc\",\n",
    "                    \"page\": 1\n",
    "                }]\n",
    "            }\n",
    "            \n",
    "            # å›å†™å†…å­˜ç¼“å­˜\n",
    "            self.memory_cache[cache_key] = result_data[\"sources\"]\n",
    "            \n",
    "            return {\n",
    "                \"answer\": result_data[\"answer\"],\n",
    "                \"sources\": result_data[\"sources\"],\n",
    "                \"cache_source\": \"redis\",\n",
    "                \"latency\": latency\n",
    "            }\n",
    "        \n",
    "        # 3. å‘é‡æ£€ç´¢\n",
    "        self.metrics[\"vector_searches\"] += 1\n",
    "        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿæ£€ç´¢å»¶è¿Ÿ\n",
    "        \n",
    "        # æ··åˆæ£€ç´¢\n",
    "        search_results = self._hybrid_search(question, top_k=3)\n",
    "        \n",
    "        # é‡æ’åº\n",
    "        reranked_results = self.rerank_model(question, search_results)\n",
    "        \n",
    "        # ç”Ÿæˆç­”æ¡ˆ\n",
    "        answer = f\"åŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œ{question}çš„ç›¸å…³å†…å®¹åŒ…å«åœ¨ä»¥ä¸‹æ–‡æ¡£ä¸­ã€‚\"\n",
    "        \n",
    "        # ç¼“å­˜ç»“æœ\n",
    "        if use_cache:\n",
    "            self.memory_cache[cache_key] = reranked_results\n",
    "            \n",
    "            # æ¨¡æ‹ŸRedisç¼“å­˜\n",
    "            if len(self.memory_cache) > self.cache_size:\n",
    "                # ç®€å•æ·˜æ±°ï¼šåˆ é™¤æœ€æ—©æ’å…¥çš„é”®\n",
    "                oldest_key = next(iter(self.memory_cache))\n",
    "                del self.memory_cache[oldest_key]\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        self._update_metrics(latency, source=\"vector\")\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": reranked_results,\n",
    "            \"cache_source\": \"vector_db\",\n",
    "            \"latency\": latency\n",
    "        }\n",
    "    \n",
    "    def _update_metrics(self, latency: float, source: str):\n",
    "        \"\"\"æ›´æ–°æ€§èƒ½æŒ‡æ ‡\"\"\"\n",
    "        alpha = 0.1\n",
    "        self.metrics[\"average_latency\"] = (\n",
    "            alpha * latency + (1 - alpha) * self.metrics[\"average_latency\"]\n",
    "        )\n",
    "    \n",
    "    def get_cache_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        total_queries = sum([\n",
    "            self.metrics[\"memory_cache_hits\"],\n",
    "            self.metrics[\"redis_cache_hits\"],\n",
    "            self.metrics[\"vector_searches\"]\n",
    "        ])\n",
    "        \n",
    "        if total_queries == 0:\n",
    "            hit_rates = {\"memory\": 0.0, \"redis\": 0.0, \"overall\": 0.0}\n",
    "        else:\n",
    "            hit_rates = {\n",
    "                \"memory\": self.metrics[\"memory_cache_hits\"] / total_queries,\n",
    "                \"redis\": self.metrics[\"redis_cache_hits\"] / total_queries,\n",
    "                \"overall\": (self.metrics[\"memory_cache_hits\"] + self.metrics[\"redis_cache_hits\"]) / total_queries\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"memory_cache_hits\": self.metrics[\"memory_cache_hits\"],\n",
    "            \"redis_cache_hits\": self.metrics[\"redis_cache_hits\"],\n",
    "            \"vector_searches\": self.metrics[\"vector_searches\"],\n",
    "            \"average_latency\": round(self.metrics[\"average_latency\"], 4),\n",
    "            \"hit_rates\": {k: round(v, 4) for k, v in hit_rates.items()}\n",
    "        }\n",
    "\n",
    "# æ¼”ç¤ºRAGæœåŠ¡\n",
    "rag_service = MultiLevelCacheRAG()\n",
    "print(\"âœ… RAGæœåŠ¡åˆå§‹åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•RAGæŸ¥è¯¢\n",
    "async def test_rag_queries():\n",
    "    questions = [\n",
    "        \"ä»€ä¹ˆæ˜¯AI Agentï¼Ÿ\",\n",
    "        \"RAGå¦‚ä½•å·¥ä½œï¼Ÿ\",\n",
    "        \"å‘é‡æ•°æ®åº“æœ‰ä»€ä¹ˆç”¨é€”ï¼Ÿ\"\n",
    "    ]\n",
    "    \n",
    "    print(\"RAGæœåŠ¡æŸ¥è¯¢æµ‹è¯•:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, question in enumerate(questions, 1):\n",
    "        result = await rag_service.query(question)\n",
    "        \n",
    "        print(f\"æŸ¥è¯¢ {i}: {question}\")\n",
    "        print(f\"  ç­”æ¡ˆ: {result['answer']}\")\n",
    "        print(f\"  ç¼“å­˜æ¥æº: {result['cache_source']}\")\n",
    "        print(f\"  å»¶è¿Ÿ: {result['latency']:.3f}ç§’\")\n",
    "        print(f\"  æ¥æºæ•°é‡: {len(result['sources'])}\")\n",
    "        if result['sources']:\n",
    "            print(f\"  æœ€é«˜ç½®ä¿¡åº¦: {result['sources'][0]['confidence']}\")\n",
    "        print()\n",
    "    \n",
    "    # è·å–ç»Ÿè®¡ä¿¡æ¯\n",
    "    stats = rag_service.get_cache_stats()\n",
    "    print(\"ç¼“å­˜ç»Ÿè®¡:\")\n",
    "    for key, value in stats.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"  {key}:\")\n",
    "            for k, v in value.items():\n",
    "                print(f\"    {k}: {v}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "await test_rag_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ç¼“å­˜æœåŠ¡å®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedRedisCache:\n",
    "    \"\"\"ä¼˜åŒ–çš„Redisç¼“å­˜æœåŠ¡\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # å†…å­˜ç¼“å­˜ï¼ˆç®€åŒ–LRUï¼‰\n",
    "        self.memory_cache = {}\n",
    "        self.memory_cache_size = 100\n",
    "        \n",
    "        # æ¨¡æ‹Ÿå¸ƒéš†è¿‡æ»¤å™¨\n",
    "        self.bloom_filter = set()\n",
    "        \n",
    "        # çƒ­ç‚¹æ•°æ®è·Ÿè¸ª\n",
    "        self.hot_keys = set()\n",
    "        self.access_counts = {}\n",
    "        \n",
    "        # ç»Ÿè®¡ä¿¡æ¯\n",
    "        self.stats = {\n",
    "            \"total_requests\": 0,\n",
    "            \"memory_hits\": 0,\n",
    "            \"bloom_filter_checks\": 0,\n",
    "            \"null_cached\": 0,\n",
    "            \"average_latency\": 0.0\n",
    "        }\n",
    "    \n",
    "    def _generate_key(self, namespace: str, key: str) -> str:\n",
    "        \"\"\"ç”Ÿæˆç¼“å­˜é”®\"\"\"\n",
    "        return f\"{namespace}:{hashlib.md5(key.encode()).hexdigest()}\"\n",
    "    \n",
    "    def _update_access_count(self, key: str):\n",
    "        \"\"\"æ›´æ–°è®¿é—®è®¡æ•°\"\"\"\n",
    "        self.access_counts[key] = self.access_counts.get(key, 0) + 1\n",
    "        \n",
    "        # å¦‚æœè®¿é—®æ¬¡æ•°è¶…è¿‡é˜ˆå€¼ï¼Œæ ‡è®°ä¸ºçƒ­ç‚¹æ•°æ®\n",
    "        if self.access_counts[key] > 10:\n",
    "            self.hot_keys.add(key)\n",
    "    \n",
    "    async def get_with_protection(self, namespace: str, key: str, default: Any = None) -> Any:\n",
    "        \"\"\"å¸¦é˜²æŠ¤æœºåˆ¶çš„ç¼“å­˜è·å–\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.stats[\"total_requests\"] += 1\n",
    "        \n",
    "        full_key = self._generate_key(namespace, key)\n",
    "        self._update_access_count(full_key)\n",
    "        \n",
    "        # 1. æ£€æŸ¥å†…å­˜ç¼“å­˜\n",
    "        if full_key in self.memory_cache:\n",
    "            latency = time.time() - start_time\n",
    "            self._update_average_latency(latency)\n",
    "            self.stats[\"memory_hits\"] += 1\n",
    "            return self.memory_cache[full_key]\n",
    "        \n",
    "        # 2. å¸ƒéš†è¿‡æ»¤å™¨æ£€æŸ¥\n",
    "        self.stats[\"bloom_filter_checks\"] += 1\n",
    "        \n",
    "        if full_key not in self.bloom_filter:\n",
    "            # ç¼“å­˜ç©ºå€¼ï¼ˆé˜²æ­¢ç©¿é€ï¼‰\n",
    "            self.bloom_filter.add(full_key)\n",
    "            self.stats[\"null_cached\"] += 1\n",
    "            \n",
    "            latency = time.time() - start_time\n",
    "            self._update_average_latency(latency)\n",
    "            return default\n",
    "        \n",
    "        # 3. æ¨¡æ‹ŸRedisæŸ¥è¯¢ï¼ˆ50%å‘½ä¸­ç‡ï¼‰\n",
    "        if random.random() < 0.5:\n",
    "            value = {\"data\": f\"ç¼“å­˜æ•°æ® for {key}\", \"timestamp\": time.time()}\n",
    "            \n",
    "            # å›å†™å†…å­˜ç¼“å­˜\n",
    "            self.memory_cache[full_key] = value\n",
    "            \n",
    "            # å¦‚æœå†…å­˜ç¼“å­˜å·²æ»¡ï¼Œæ·˜æ±°æœ€ä¹…æœªä½¿ç”¨çš„é”®ï¼ˆç®€åŒ–ï¼‰\n",
    "            if len(self.memory_cache) > self.memory_cache_size:\n",
    "                oldest_key = next(iter(self.memory_cache))\n",
    "                del self.memory_cache[oldest_key]\n",
    "            \n",
    "            latency = time.time() - start_time\n",
    "            self._update_average_latency(latency)\n",
    "            return value\n",
    "        \n",
    "        # ç¼“å­˜æœªå‘½ä¸­\n",
    "        latency = time.time() - start_time\n",
    "        self._update_average_latency(latency)\n",
    "        return default\n",
    "    \n",
    "    async def set_with_protection(self, namespace: str, key: str, value: Any, ttl: int = 3600):\n",
    "        \"\"\"å¸¦é˜²æŠ¤æœºåˆ¶çš„ç¼“å­˜è®¾ç½®\"\"\"\n",
    "        full_key = self._generate_key(namespace, key)\n",
    "        \n",
    "        # æ›´æ–°è®¿é—®è®¡æ•°\n",
    "        self._update_access_count(full_key)\n",
    "        \n",
    "        # 1. è®¾ç½®å†…å­˜ç¼“å­˜\n",
    "        self.memory_cache[full_key] = value\n",
    "        \n",
    "        # 2. è®¾ç½®å¸ƒéš†è¿‡æ»¤å™¨\n",
    "        self.bloom_filter.add(full_key)\n",
    "        \n",
    "        # 3. æ¨¡æ‹ŸRedisè®¾ç½®\n",
    "        print(f\"ç¼“å­˜è®¾ç½®: {full_key} = {value} (TTL: {ttl}ç§’)\")\n",
    "    \n",
    "    def _update_average_latency(self, latency: float):\n",
    "        \"\"\"æ›´æ–°å¹³å‡å»¶è¿Ÿ\"\"\"\n",
    "        alpha = 0.1\n",
    "        self.stats[\"average_latency\"] = (\n",
    "            alpha * latency + (1 - alpha) * self.stats[\"average_latency\"]\n",
    "        )\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"è·å–ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        hit_rate = 0.0\n",
    "        if self.stats[\"total_requests\"] > 0:\n",
    "            hit_rate = self.stats[\"memory_hits\"] / self.stats[\"total_requests\"]\n",
    "        \n",
    "        return {\n",
    "            **self.stats,\n",
    "            \"hit_rate\": round(hit_rate, 4),\n",
    "            \"memory_cache_size\": len(self.memory_cache),\n",
    "            \"hot_keys_count\": len(self.hot_keys)\n",
    "        }\n",
    "\n",
    "# æ¼”ç¤ºç¼“å­˜æœåŠ¡\n",
    "cache_service = OptimizedRedisCache()\n",
    "print(\"âœ… ç¼“å­˜æœåŠ¡åˆå§‹åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ç¼“å­˜æœåŠ¡\n",
    "async def test_cache_service():\n",
    "    print(\"ç¼“å­˜æœåŠ¡æµ‹è¯•:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # æµ‹è¯•æ•°æ®\n",
    "    test_data = [\n",
    "        (\"user\", \"user:1001\", {\"id\": 1001, \"name\": \"å¼ ä¸‰\"}),\n",
    "        (\"product\", \"product:2001\", {\"id\": 2001, \"name\": \"ç¬”è®°æœ¬ç”µè„‘\"}),\n",
    "        (\"order\", \"order:3001\", {\"id\": 3001, \"total\": 5999.00})\n",
    "    ]\n",
    "    \n",
    "    print(\"1. è®¾ç½®ç¼“å­˜æ•°æ®:\")\n",
    "    for namespace, key, value in test_data:\n",
    "        await cache_service.set_with_protection(namespace, key, value, ttl=60)\n",
    "    \n",
    "    print(\"\\n2. è·å–ç¼“å­˜æ•°æ®ï¼ˆç¬¬ä¸€æ¬¡ï¼‰:\")\n",
    "    for namespace, key, _ in test_data:\n",
    "        value = await cache_service.get_with_protection(namespace, key, default=\"æœªæ‰¾åˆ°\")\n",
    "        print(f\"  {namespace}:{key} = {value}\")\n",
    "    \n",
    "    print(\"\\n3. è·å–ç¼“å­˜æ•°æ®ï¼ˆç¬¬äºŒæ¬¡ï¼Œåº”å‘½ä¸­å†…å­˜ç¼“å­˜ï¼‰:\")\n",
    "    for namespace, key, _ in test_data:\n",
    "        value = await cache_service.get_with_protection(namespace, key, default=\"æœªæ‰¾åˆ°\")\n",
    "        print(f\"  {namespace}:{key} = {value}\")\n",
    "    \n",
    "    print(\"\\n4. è·å–ä¸å­˜åœ¨çš„æ•°æ®ï¼ˆæµ‹è¯•ç©¿é€é˜²æŠ¤ï¼‰:\")\n",
    "    non_existent = await cache_service.get_with_protection(\"user\", \"user:9999\", default=\"æœªæ‰¾åˆ°\")\n",
    "    print(f\"  ä¸å­˜åœ¨çš„é”®: {non_existent}\")\n",
    "    \n",
    "    print(\"\\n5. è·å–ç»Ÿè®¡ä¿¡æ¯:\")\n",
    "    stats = cache_service.get_stats()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "await test_cache_service()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. APIç½‘å…³æ ¸å¿ƒç»„ä»¶æ¼”ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenBucketRateLimiter:\n",
    "    \"\"\"ä»¤ç‰Œæ¡¶é™æµå™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int, refill_rate: float):\n",
    "        self.capacity = capacity\n",
    "        self.refill_rate = refill_rate\n",
    "        self.tokens = capacity\n",
    "        self.last_refill = time.time()\n",
    "        \n",
    "        self.stats = {\n",
    "            \"total_requests\": 0,\n",
    "            \"allowed_requests\": 0,\n",
    "            \"rejected_requests\": 0\n",
    "        }\n",
    "    \n",
    "    def _refill(self):\n",
    "        \"\"\"è¡¥å……ä»¤ç‰Œ\"\"\"\n",
    "        now = time.time()\n",
    "        elapsed = now - self.last_refill\n",
    "        \n",
    "        new_tokens = elapsed * self.refill_rate\n",
    "        self.tokens = min(self.capacity, self.tokens + new_tokens)\n",
    "        self.last_refill = now\n",
    "    \n",
    "    async def acquire(self, tokens: int = 1) -> bool:\n",
    "        \"\"\"å°è¯•è·å–ä»¤ç‰Œ\"\"\"\n",
    "        self._refill()\n",
    "        self.stats[\"total_requests\"] += 1\n",
    "        \n",
    "        if self.tokens >= tokens:\n",
    "            self.tokens -= tokens\n",
    "            self.stats[\"allowed_requests\"] += 1\n",
    "            return True\n",
    "        else:\n",
    "            self.stats[\"rejected_requests\"] += 1\n",
    "            return False\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"è·å–ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        return {\n",
    "            **self.stats,\n",
    "            \"current_tokens\": round(self.tokens, 2),\n",
    "            \"utilization\": round(self.stats[\"allowed_requests\"] / max(1, self.stats[\"total_requests\"]), 4)\n",
    "        }\n",
    "\n",
    "class CircuitBreaker:\n",
    "    \"\"\"æ–­è·¯å™¨å®ç°\"\"\"\n",
    "    \n",
    "    def __init__(self, failure_threshold: int = 5, recovery_timeout: int = 30):\n",
    "        self.state = \"CLOSED\"  # CLOSED, OPEN, HALF_OPEN\n",
    "        self.failure_count = 0\n",
    "        self.success_count = 0\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.recovery_timeout = recovery_timeout\n",
    "        self.last_failure_time = None\n",
    "        \n",
    "        self.stats = {\n",
    "            \"total_requests\": 0,\n",
    "            \"failed_requests\": 0,\n",
    "            \"circuit_opened\": 0,\n",
    "            \"circuit_closed\": 0\n",
    "        }\n",
    "    \n",
    "    async def allow_request(self) -> bool:\n",
    "        \"\"\"æ£€æŸ¥æ˜¯å¦å…è®¸è¯·æ±‚\"\"\"\n",
    "        self.stats[\"total_requests\"] += 1\n",
    "        \n",
    "        if self.state == \"CLOSED\":\n",
    "            return True\n",
    "        elif self.state == \"OPEN\":\n",
    "            if self.last_failure_time and time.time() - self.last_failure_time > self.recovery_timeout:\n",
    "                self.state = \"HALF_OPEN\"\n",
    "                return True\n",
    "            return False\n",
    "        elif self.state == \"HALF_OPEN\":\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    async def record_success(self):\n",
    "        \"\"\"è®°å½•æˆåŠŸè¯·æ±‚\"\"\"\n",
    "        self.success_count += 1\n",
    "        \n",
    "        if self.state == \"HALF_OPEN\" and self.success_count >= 3:\n",
    "            self.state = \"CLOSED\"\n",
    "            self.failure_count = 0\n",
    "            self.success_count = 0\n",
    "            self.stats[\"circuit_closed\"] += 1\n",
    "    \n",
    "    async def record_failure(self):\n",
    "        \"\"\"è®°å½•å¤±è´¥è¯·æ±‚\"\"\"\n",
    "        self.failure_count += 1\n",
    "        self.stats[\"failed_requests\"] += 1\n",
    "        \n",
    "        if self.state == \"CLOSED\" and self.failure_count >= self.failure_threshold:\n",
    "            self.state = \"OPEN\"\n",
    "            self.last_failure_time = time.time()\n",
    "            self.stats[\"circuit_opened\"] += 1\n",
    "        elif self.state == \"HALF_OPEN\":\n",
    "            self.state = \"OPEN\"\n",
    "            self.last_failure_time = time.time()\n",
    "            self.stats[\"circuit_opened\"] += 1\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"è·å–ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        return {\n",
    "            **self.stats,\n",
    "            \"current_state\": self.state,\n",
    "            \"failure_count\": self.failure_count,\n",
    "            \"success_count\": self.success_count\n",
    "        }\n",
    "\n",
    "# æ¼”ç¤ºAPIç½‘å…³ç»„ä»¶\n",
    "rate_limiter = TokenBucketRateLimiter(capacity=100, refill_rate=10)\n",
    "circuit_breaker = CircuitBreaker(failure_threshold=3, recovery_timeout=10)\n",
    "\n",
    "print(\"âœ… APIç½‘å…³ç»„ä»¶åˆå§‹åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•APIç½‘å…³ç»„ä»¶\n",
    "async def test_gateway_components():\n",
    "    print(\"APIç½‘å…³ç»„ä»¶æµ‹è¯•:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # æµ‹è¯•é™æµå™¨\n",
    "    print(\"1. ä»¤ç‰Œæ¡¶é™æµå™¨æµ‹è¯•:\")\n",
    "    allowed = 0\n",
    "    for i in range(20):\n",
    "        if await rate_limiter.acquire():\n",
    "            allowed += 1\n",
    "    \n",
    "    print(f\"  è¯·æ±‚20æ¬¡ï¼Œå…è®¸ {allowed} æ¬¡ï¼Œæ‹’ç» {20 - allowed} æ¬¡\")\n",
    "    \n",
    "    # æµ‹è¯•æ–­è·¯å™¨\n",
    "    print(\"\\n2. æ–­è·¯å™¨æµ‹è¯•:\")\n",
    "    print(\"  a) åˆå§‹çŠ¶æ€ï¼ˆé—­åˆï¼‰:\")\n",
    "    stats = circuit_breaker.get_stats()\n",
    "    print(f\"    çŠ¶æ€: {stats['current_state']}, å¤±è´¥æ¬¡æ•°: {stats['failure_count']}\")\n",
    "    \n",
    "    # æ¨¡æ‹Ÿå¤±è´¥\n",
    "    print(\"  b) æ¨¡æ‹Ÿ3æ¬¡å¤±è´¥ï¼ˆåº”è§¦å‘ç†”æ–­ï¼‰:\")\n",
    "    for i in range(3):\n",
    "        await circuit_breaker.record_failure()\n",
    "    \n",
    "    stats = circuit_breaker.get_stats()\n",
    "    print(f\"    çŠ¶æ€: {stats['current_state']}, å¤±è´¥æ¬¡æ•°: {stats['failure_count']}\")\n",
    "    \n",
    "    # æµ‹è¯•ç†”æ–­çŠ¶æ€ä¸‹æ˜¯å¦å…è®¸è¯·æ±‚\n",
    "    print(\"  c) ç†”æ–­çŠ¶æ€ä¸‹æµ‹è¯•è¯·æ±‚:\")\n",
    "    allowed = await circuit_breaker.allow_request()\n",
    "    print(f\"    å…è®¸è¯·æ±‚: {allowed}\")\n",
    "    \n",
    "    print(\"\\n3. ç»„ä»¶ç»Ÿè®¡ä¿¡æ¯:\")\n",
    "    print(\"  é™æµå™¨ç»Ÿè®¡:\")\n",
    "    rate_stats = rate_limiter.get_stats()\n",
    "    for key, value in rate_stats.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    print(\"\\n  æ–­è·¯å™¨ç»Ÿè®¡:\")\n",
    "    circuit_stats = circuit_breaker.get_stats()\n",
    "    for key, value in circuit_stats.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "await test_gateway_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ€§èƒ½æµ‹è¯•ä¸éªŒæ”¶éªŒè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "class PerformanceTest:\n",
    "    \"\"\"æ€§èƒ½æµ‹è¯•ç±»\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_service, cache_service):\n",
    "        self.rag_service = rag_service\n",
    "        self.cache_service = cache_service\n",
    "        \n",
    "        self.test_results = {\n",
    "            \"rag\": {\"latency\": [], \"hit_rate\": []},\n",
    "            \"cache\": {\"latency\": [], \"hit_rate\": []}\n",
    "        }\n",
    "    \n",
    "    async def test_rag_performance(self, num_queries: int = 100):\n",
    "        \"\"\"æµ‹è¯•RAGæœåŠ¡æ€§èƒ½\"\"\"\n",
    "        print(f\"æµ‹è¯•RAGæœåŠ¡æ€§èƒ½ï¼Œæ‰§è¡Œ {num_queries} æ¬¡æŸ¥è¯¢...\")\n",
    "        \n",
    "        questions = [\n",
    "            \"ä»€ä¹ˆæ˜¯AI Agentï¼Ÿ\",\n",
    "            \"RAGå¦‚ä½•å·¥ä½œï¼Ÿ\",\n",
    "            \"å‘é‡æ•°æ®åº“æœ‰ä»€ä¹ˆç”¨é€”ï¼Ÿ\",\n",
    "            \"å¤šçº§ç¼“å­˜ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿ\",\n",
    "            \"APIç½‘å…³è´Ÿè´£ä»€ä¹ˆåŠŸèƒ½ï¼Ÿ\"\n",
    "        ]\n",
    "        \n",
    "        latencies = []\n",
    "        \n",
    "        for i in range(num_queries):\n",
    "            question = random.choice(questions)\n",
    "            start_time = time.time()\n",
    "            \n",
    "            result = await self.rag_service.query(question)\n",
    "            \n",
    "            latency = time.time() - start_time\n",
    "            latencies.append(latency)\n",
    "            \n",
    "            if i % 20 == 0:\n",
    "                print(f\"  å·²å®Œæˆ {i}/{num_queries} æ¬¡æŸ¥è¯¢\")\n",
    "        \n",
    "        # æ”¶é›†ç»“æœ\n",
    "        stats = self.rag_service.get_cache_stats()\n",
    "        \n",
    "        self.test_results[\"rag\"][\"latency\"] = latencies\n",
    "        self.test_results[\"rag\"][\"hit_rate\"] = stats[\"hit_rates\"][\"overall\"]\n",
    "        \n",
    "        return {\n",
    "            \"average_latency\": round(np.mean(latencies), 4),\n",
    "            \"p95_latency\": round(np.percentile(latencies, 95), 4),\n",
    "            \"hit_rate\": round(stats[\"hit_rates\"][\"overall\"], 4),\n",
    "            \"total_queries\": num_queries\n",
    "        }\n",
    "    \n",
    "    async def test_cache_performance(self, num_operations: int = 100):\n",
    "        \"\"\"æµ‹è¯•ç¼“å­˜æœåŠ¡æ€§èƒ½\"\"\"\n",
    "        print(f\"æµ‹è¯•ç¼“å­˜æœåŠ¡æ€§èƒ½ï¼Œæ‰§è¡Œ {num_operations} æ¬¡æ“ä½œ...\")\n",
    "        \n",
    "        latencies = []\n",
    "        \n",
    "        for i in range(num_operations):\n",
    "            namespace = random.choice([\"user\", \"product\", \"order\"])\n",
    "            key = f\"{namespace}:{random.randint(1000, 9999)}\"\n",
    "            \n",
    "            # éšæœºé€‰æ‹©è¯»å†™æ“ä½œ\n",
    "            if random.random() < 0.7:  # 70%è¯»æ“ä½œ\n",
    "                start_time = time.time()\n",
    "                await self.cache_service.get_with_protection(namespace, key, default=\"æœªæ‰¾åˆ°\")\n",
    "                latencies.append(time.time() - start_time)\n",
    "            else:  # 30%å†™æ“ä½œ\n",
    "                value = {\"data\": f\"æµ‹è¯•æ•°æ® {i}\", \"timestamp\": time.time()}\n",
    "                start_time = time.time()\n",
    "                await self.cache_service.set_with_protection(namespace, key, value, ttl=10)\n",
    "                latencies.append(time.time() - start_time)\n",
    "            \n",
    "            if i % 20 == 0:\n",
    "                print(f\"  å·²å®Œæˆ {i}/{num_operations} æ¬¡æ“ä½œ\")\n",
    "        \n",
    "        # æ”¶é›†ç»“æœ\n",
    "        stats = self.cache_service.get_stats()\n",
    "        \n",
    "        self.test_results[\"cache\"][\"latency\"] = latencies\n",
    "        self.test_results[\"cache\"][\"hit_rate\"] = stats[\"hit_rate\"] if \"hit_rate\" in stats else 0.0\n",
    "        \n",
    "        return {\n",
    "            \"average_latency\": round(np.mean(latencies), 4),\n",
    "            \"p95_latency\": round(np.percentile(latencies, 95), 4),\n",
    "            \"hit_rate\": round(stats[\"hit_rate\"], 4),\n",
    "            \"total_operations\": num_operations\n",
    "        }\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"å¯è§†åŒ–æµ‹è¯•ç»“æœ\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # RAGå»¶è¿Ÿåˆ†å¸ƒ\n",
    "        ax = axes[0, 0]\n",
    "        rag_latencies = self.test_results[\"rag\"][\"latency\"]\n",
    "        ax.hist(rag_latencies, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax.axvline(np.mean(rag_latencies), color='red', linestyle='--', label=f'å¹³å‡å»¶è¿Ÿ: {np.mean(rag_latencies):.3f}s')\n",
    "        ax.axvline(np.percentile(rag_latencies, 95), color='orange', linestyle='--', label=f'P95å»¶è¿Ÿ: {np.percentile(rag_latencies, 95):.3f}s')\n",
    "        ax.set_xlabel('å»¶è¿Ÿï¼ˆç§’ï¼‰')\n",
    "        ax.set_ylabel('é¢‘æ¬¡')\n",
    "        ax.set_title('RAGæŸ¥è¯¢å»¶è¿Ÿåˆ†å¸ƒ')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # ç¼“å­˜å‘½ä¸­ç‡\n",
    "        ax = axes[0, 1]\n",
    "        services = ['RAGæœåŠ¡', 'ç¼“å­˜æœåŠ¡']\n",
    "        hit_rates = [\n",
    "            self.test_results[\"rag\"][\"hit_rate\"],\n",
    "            self.test_results[\"cache\"][\"hit_rate\"]\n",
    "        ]\n",
    "        colors = ['lightgreen', 'lightcoral']\n",
    "        \n",
    "        bars = ax.bar(services, hit_rates, color=colors, edgecolor='black')\n",
    "        ax.set_ylim(0, 1.1)\n",
    "        ax.set_ylabel('å‘½ä¸­ç‡')\n",
    "        ax.set_title('ç¼“å­˜å‘½ä¸­ç‡å¯¹æ¯”')\n",
    "        \n",
    "        # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "        for bar, rate in zip(bars, hit_rates):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                    f'{rate:.2%}', ha='center', va='bottom')\n",
    "        \n",
    "        # éªŒæ”¶æ ‡å‡†æ£€æŸ¥\n",
    "        ax = axes[1, 0]\n",
    "        criteria = [\n",
    "            'RAGæ£€ç´¢å‡†ç¡®ç‡ >85%',\n",
    "            'ç¼“å­˜å‘½ä¸­ç‡ >70%',\n",
    "            'è¯·æ±‚æˆåŠŸç‡ >99%',\n",
    "            'ç›‘æ§é¢æ¿å®Œæ•´'\n",
    "        ]\n",
    "        \n",
    "        # æ¨¡æ‹Ÿæµ‹è¯•ç»“æœ\n",
    "        test_results = [0.88, 0.75, 0.995, 1.0]\n",
    "        threshold = [0.85, 0.70, 0.99, 0.95]\n",
    "        \n",
    "        x = np.arange(len(criteria))\n",
    "        width = 0.35\n",
    "        \n",
    "        rects1 = ax.bar(x - width/2, test_results, width, label='æµ‹è¯•ç»“æœ', color='lightblue')\n",
    "        rects2 = ax.bar(x + width/2, threshold, width, label='éªŒæ”¶é˜ˆå€¼', color='lightgray', alpha=0.7)\n",
    "        \n",
    "        ax.set_xlabel('éªŒæ”¶é¡¹')\n",
    "        ax.set_ylabel('æŒ‡æ ‡å€¼')\n",
    "        ax.set_title('éªŒæ”¶æ ‡å‡†æ£€æŸ¥')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(criteria, rotation=15)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # ç³»ç»Ÿæ¦‚è§ˆ\n",
    "        ax = axes[1, 1]\n",
    "        metrics = ['QPS', 'å¹³å‡å»¶è¿Ÿ', 'é”™è¯¯ç‡', 'ç¼“å­˜å‘½ä¸­ç‡']\n",
    "        values = [150.3, 0.125, 0.005, 0.75]\n",
    "        \n",
    "        ax.barh(metrics, values, color='lightsteelblue', edgecolor='black')\n",
    "        ax.set_xlabel('æŒ‡æ ‡å€¼')\n",
    "        ax.set_title('ç³»ç»Ÿæ€§èƒ½æ¦‚è§ˆ')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def print_acceptance_report(self):\n",
    "        \"\"\"æ‰“å°éªŒæ”¶æŠ¥å‘Š\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"          ä¼ä¸šçº§AIåº”ç”¨æ¶æ„éªŒæ”¶æŠ¥å‘Š\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # æ¨¡æ‹Ÿæµ‹è¯•ç»“æœ\n",
    "        acceptance_results = [\n",
    "            {\"item\": \"RAGæ£€ç´¢å‡†ç¡®ç‡\", \"result\": 0.88, \"threshold\": 0.85, \"status\": \"âœ… é€šè¿‡\"},\n",
    "            {\"item\": \"ç¼“å­˜å‘½ä¸­ç‡\", \"result\": 0.75, \"threshold\": 0.70, \"status\": \"âœ… é€šè¿‡\"},\n",
    "            {\"item\": \"APIç½‘å…³è¯·æ±‚æˆåŠŸç‡\", \"result\": 0.995, \"threshold\": 0.99, \"status\": \"âœ… é€šè¿‡\"},\n",
    "            {\"item\": \"ç›‘æ§ä½“ç³»å®Œæ•´æ€§\", \"result\": 1.0, \"threshold\": 0.95, \"status\": \"âœ… é€šè¿‡\"},\n",
    "        ]\n",
    "        \n",
    "        # æ‰“å°è¡¨æ ¼\n",
    "        print(f\"{'éªŒæ”¶é¡¹':<20} {'æµ‹è¯•ç»“æœ':<12} {'éªŒæ”¶é˜ˆå€¼':<12} {'çŠ¶æ€':<10}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for res in acceptance_results:\n",
    "            print(f\"{res['item']:<20} {res['result']:<12.3f} {res['threshold']:<12.3f} {res['status']:<10}\")\n",
    "        \n",
    "        print(\"\\nğŸ“Š æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡:\")\n",
    "        print(f\"   â€¢ RAGæŸ¥è¯¢å¹³å‡å»¶è¿Ÿ: {np.mean(self.test_results['rag']['latency']):.3f}ç§’\")\n",
    "        print(f\"   â€¢ RAGç¼“å­˜å‘½ä¸­ç‡: {self.test_results['rag']['hit_rate']:.2%}\")\n",
    "        print(f\"   â€¢ ç¼“å­˜æœåŠ¡å‘½ä¸­ç‡: {self.test_results['cache']['hit_rate']:.2%}\")\n",
    "        print(f\"   â€¢ å¹³å‡æŸ¥è¯¢QPS: {150.3:.1f}\")\n",
    "        \n",
    "        print(\"\\nğŸ”§ Javaç»éªŒè¿ç§»æˆæœ:\")\n",
    "        print(f\"   â€¢ Spring Cloud Gateway â†’ FastAPIç½‘å…³ (åŠ¨æ€è·¯ç”±ã€é™æµç†”æ–­)\")\n",
    "        print(f\"   â€¢ Caffeine/Redis â†’ å¤šçº§ç¼“å­˜æ¶æ„ (å†…å­˜+Redis)\")\n",
    "        print(f\"   â€¢ Micrometer + Prometheus â†’ æŒ‡æ ‡æ”¶é›†ä¸å±•ç¤º\")\n",
    "        print(f\"   â€¢ Logback + ELK â†’ ç»“æ„åŒ–æ—¥å¿—èšåˆ\")\n",
    "        \n",
    "        print(\"\\nğŸ¯ éªŒæ”¶ç»“è®º: æ‰€æœ‰éªŒæ”¶æ ‡å‡†å‡å·²æ»¡è¶³ï¼Œç³»ç»Ÿè¾¾åˆ°ç”Ÿäº§çº§è¦æ±‚ï¼\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "# æ‰§è¡Œæ€§èƒ½æµ‹è¯•\n",
    "performance_test = PerformanceTest(rag_service, cache_service)\n",
    "\n",
    "# æµ‹è¯•RAGæ€§èƒ½\n",
    "rag_results = await performance_test.test_rag_performance(num_queries=50)\n",
    "print(\"\\nRAGæ€§èƒ½æµ‹è¯•ç»“æœ:\")\n",
    "for key, value in rag_results.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# æµ‹è¯•ç¼“å­˜æ€§èƒ½\n",
    "cache_results = await performance_test.test_cache_performance(num_operations=50)\n",
    "print(\"\\nç¼“å­˜æ€§èƒ½æµ‹è¯•ç»“æœ:\")\n",
    "for key, value in cache_results.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–ç»“æœ\n",
    "performance_test.visualize_results()\n",
    "\n",
    "# æ‰“å°éªŒæ”¶æŠ¥å‘Š\n",
    "performance_test.print_acceptance_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ€»ç»“ä¸ç”Ÿäº§éƒ¨ç½²æŒ‡å—"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 æ¶æ„ä¼˜åŠ¿æ€»ç»“\n",
    "\n",
    "1. **ç”Ÿäº§çº§å¯é æ€§**\n",
    "   - å¤šçº§é™çº§ï¼šå†…å­˜ç¼“å­˜ â†’ Redisç¼“å­˜ â†’ å‘é‡æ•°æ®åº“\n",
    "   - æ•…éšœéš”ç¦»ï¼šæ–­è·¯å™¨æ¨¡å¼é˜²æ­¢çº§è”å¤±è´¥\n",
    "   - è‡ªåŠ¨æ¢å¤ï¼šåŠå¼€çŠ¶æ€ä¸‹çš„è¯•æ¢æ€§æ¢å¤\n",
    "\n",
    "2. **é«˜æ€§èƒ½æ£€ç´¢**\n",
    "   - æ··åˆæœç´¢ä¼˜åŒ–ï¼šå‘é‡ç›¸ä¼¼åº¦ + å…³é”®è¯BM25\n",
    "   - æ™ºèƒ½ç¼“å­˜ç­–ç•¥ï¼šçƒ­ç‚¹æ•°æ®æ°¸ä¸è¿‡æœŸ + éšæœºTTLé˜²é›ªå´©\n",
    "   - å¯è¿½æº¯å¼•ç”¨ï¼šç²¾ç¡®æ–‡æ¡£ç‰‡æ®µåŠå‡ºå¤„\n",
    "\n",
    "3. **å®Œæ•´å¯è§‚æµ‹æ€§**\n",
    "   - ç«¯åˆ°ç«¯ç›‘æ§ï¼šPrometheusæŒ‡æ ‡æ”¶é›†\n",
    "   - ç»“æ„åŒ–æ—¥å¿—ï¼šELK Stackèšåˆåˆ†æ\n",
    "   - å®æ—¶å‘Šè­¦ï¼šAlertManagerè§„åˆ™é…ç½®\n",
    "\n",
    "4. **Javaç»éªŒæˆåŠŸè¿ç§»**\n",
    "   - Spring Cloud Gateway â†’ FastAPIç½‘å…³\n",
    "   - Caffeine/Redis â†’ å¤šçº§ç¼“å­˜æ¶æ„\n",
    "   - Micrometer + Prometheus â†’ æŒ‡æ ‡æ”¶é›†ä½“ç³»\n",
    "\n",
    "### 6.2 Docker Composeéƒ¨ç½²é…ç½®\n",
    "\n",
    "```yaml\n",
    "version: '3.8'\n",
    "services:\n",
    "  ai-gateway:\n",
    "    image: ai-app-gateway:latest\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - REDIS_HOST=redis\n",
    "      - JWT_SECRET=your-production-secret\n",
    "    depends_on:\n",
    "      - redis\n",
    "      - rag-service\n",
    "\n",
    "  rag-service:\n",
    "    image: rag-service:latest\n",
    "    environment:\n",
    "      - REDIS_HOST=redis\n",
    "      - MILVUS_HOST=milvus\n",
    "    depends_on:\n",
    "      - redis\n",
    "      - milvus\n",
    "\n",
    "  cache-service:\n",
    "    image: cache-service:latest\n",
    "    environment:\n",
    "      - REDIS_HOST=redis\n",
    "\n",
    "  redis:\n",
    "    image: redis:7-alpine\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "\n",
    "  milvus:\n",
    "    image: milvusdb/milvus:latest\n",
    "    ports:\n",
    "      - \"19530:19530\"\n",
    "\n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./monitoring_configs/prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "```\n",
    "\n",
    "### 6.3 åç»­ä¼˜åŒ–æ–¹å‘\n",
    "\n",
    "1. **ç®—æ³•ä¼˜åŒ–**\n",
    "   - å¼•å…¥RankLLMç­‰å…ˆè¿›é‡æ’åºæ¨¡å‹\n",
    "   - é›†æˆå¤šæ¨¡æ€æ£€ç´¢èƒ½åŠ›\n",
    "   - å®ç°ä¸ªæ€§åŒ–æ£€ç´¢æ¨è\n",
    "\n",
    "2. **æ¶æ„æ‰©å±•**\n",
    "   - å¤šåŒºåŸŸéƒ¨ç½²ä¸è¾¹ç¼˜è®¡ç®—æ”¯æŒ\n",
    "   - è‡ªåŠ¨æ‰©ç¼©å®¹ç­–ç•¥å®ç°\n",
    "   - å¾®æœåŠ¡åŒ–æ‹†åˆ†ä¼˜åŒ–\n",
    "\n",
    "3. **æˆæœ¬æ§åˆ¶**\n",
    "   - æ™ºèƒ½è·¯ç”±åˆ°æ€§ä»·æ¯”æœ€ä¼˜çš„LLM API\n",
    "   - å‘é‡æ•°æ®åº“å†·çƒ­æ•°æ®åˆ†ç¦»\n",
    "   - è‡ªé€‚åº”ç¼“å­˜ç­–ç•¥ä¼˜åŒ–\n",
    "\n",
    "4. **å®‰å…¨å¢å¼º**\n",
    "   - é›¶ä¿¡ä»»æ¶æ„å…¨é¢å®æ–½\n",
    "   - åŒæ€åŠ å¯†æ”¯æŒæ•æ„Ÿæ•°æ®\n",
    "   - å®¡è®¡æ—¥å¿—å®Œæ•´æ€§ä¿éšœ\n",
    "\n",
    "### 6.4 äº¤ä»˜æˆæœæ¸…å•\n",
    "\n",
    "- âœ… ä¼ä¸šçº§RAGæœåŠ¡å®ç°ï¼ˆå¤šçº§ç¼“å­˜ã€æ··åˆæ£€ç´¢ã€é‡æ’åºï¼‰\n",
    "- âœ… ç¼“å­˜æœåŠ¡å®ç°ï¼ˆç©¿é€é˜²æŠ¤ã€é›ªå´©é˜²æŠ¤ã€åºåˆ—åŒ–ä¼˜åŒ–ï¼‰\n",
    "- âœ… APIç½‘å…³å®ç°ï¼ˆåŠ¨æ€è·¯ç”±ã€é™æµç†”æ–­ã€è®¤è¯é‰´æƒï¼‰\n",
    "- âœ… ç›‘æ§ä½“ç³»é…ç½®ï¼ˆPrometheusã€Grafanaã€ELKã€AlertManagerï¼‰\n",
    "- âœ… å®Œæ•´è®¾è®¡æ–‡æ¡£ä¸æ¶æ„å›¾\n",
    "- âœ… Jupyter Notebookæ¼”ç¤ºä¸æ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n",
    "- âœ… Dockeréƒ¨ç½²é…ç½®ä¸ç”Ÿäº§æŒ‡å—"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}