{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# day_01.py 核心组件使用示例\n",
    "\n",
    "本 Jupyter Notebook 提供了三个核心组件的详细使用示例：\n",
    "1. **DistributedCacheDecorator** - 分布式缓存装饰器\n",
    "2. **AsyncRAGGenerator** - 异步RAG流式生成器\n",
    "3. **ConcurrentRouter** - 并发模型路由器\n",
    "\n",
    "每个示例都包含详细注释和使用场景说明，可直接运行查看效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from day_01 import DistributedCacheDecorator, AsyncRAGGenerator, ConcurrentRouter, ModelEndpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DistributedCacheDecorator 使用示例\n",
    "\n",
    "**场景说明：**\n",
    "- 场景1: 缓存大模型API调用结果\n",
    "- 场景2: 缓存数据库查询结果\n",
    "- 场景3: 缓存计算密集型函数结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DistributedCacheDecorator 使用示例 ===\n",
      "1. 缓存大模型API调用结果:\n",
      "调用模型 gpt-4: 什么是Python?...\n",
      "同步到分布式缓存: fetch_llm_response:-822733053630420067\n",
      "首次调用耗时: 0.51秒\n",
      "结果: {'response': \"针对'什么是Python?'的AI回复\", 'tokens': 150}\n",
      "缓存命中: fetch_llm_response:-822733053630420067\n",
      "缓存命中耗时: 0.00秒\n",
      "结果: {'response': \"针对'什么是Python?'的AI回复\", 'tokens': 150}\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DistributedCacheDecorator 使用示例 ===\")\n",
    "\n",
    "# 场景1: 缓存大模型API调用结果\n",
    "print(\"1. 缓存大模型API调用结果:\")\n",
    "\n",
    "@DistributedCacheDecorator(ttl=60, cache_backend='redis')\n",
    "def fetch_llm_response(prompt: str, model: str = \"gpt-4\") -> dict:\n",
    "    \"\"\"模拟大模型API调用\"\"\"\n",
    "    print(f\"调用模型 {model}: {prompt[:50]}...\")\n",
    "    time.sleep(0.5)  # 模拟网络延迟\n",
    "    return {\"response\": f\"针对'{prompt}'的AI回复\", \"tokens\": 150}\n",
    "\n",
    "# 第一次调用，会执行函数\n",
    "start_time = time.time()\n",
    "result1 = fetch_llm_response(\"什么是Python?\")\n",
    "print(f\"首次调用耗时: {time.time() - start_time:.2f}秒\")\n",
    "print(f\"结果: {result1}\")\n",
    "\n",
    "# 第二次调用相同参数，会命中缓存\n",
    "start_time = time.time()\n",
    "result2 = fetch_llm_response(\"什么是Python?\")\n",
    "print(f\"缓存命中耗时: {time.time() - start_time:.2f}秒\")\n",
    "print(f\"结果: {result2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. 缓存数据库查询结果:\n",
      "查询用户 1001 的数据...\n",
      "同步到分布式缓存: query_database:-7274342873189353207\n",
      "查询结果: {'user_id': 1001, 'name': '用户1001', 'email': 'user1001@example.com'}\n",
      "缓存命中: query_database:-7274342873189353207\n",
      "缓存结果: {'user_id': 1001, 'name': '用户1001', 'email': 'user1001@example.com'}\n"
     ]
    }
   ],
   "source": [
    "# 场景2: 缓存数据库查询结果\n",
    "print(\"2. 缓存数据库查询结果:\")\n",
    "\n",
    "@DistributedCacheDecorator(ttl=300)  # 5分钟缓存\n",
    "def query_database(user_id: int) -> dict:\n",
    "    \"\"\"模拟数据库查询\"\"\"\n",
    "    print(f\"查询用户 {user_id} 的数据...\")\n",
    "    time.sleep(0.3)  # 模拟数据库查询延迟\n",
    "    return {\"user_id\": user_id, \"name\": f\"用户{user_id}\", \"email\": f\"user{user_id}@example.com\"}\n",
    "\n",
    "# 第一次查询\n",
    "start_time = time.time()\n",
    "result = query_database(1001)\n",
    "print(f\"查询结果: {result}\")\n",
    "print(f\"查询耗时: {time.time() - start_time:.3f}秒\")\n",
    "\n",
    "# 再次查询相同用户，会命中缓存\n",
    "start_time = time.time()\n",
    "result = query_database(1001)\n",
    "print(f\"缓存结果: {result}\")\n",
    "print(f\"缓存命中耗时: {time.time() - start_time:.3f}秒\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 场景3: 缓存计算密集型函数结果\n",
    "print(\"3. 缓存计算密集型函数结果:\")\n",
    "\n",
    "@DistributedCacheDecorator(ttl=600)  # 10分钟缓存\n",
    "def compute_expensive(x: int, y: int) -> int:\n",
    "    \"\"\"模拟计算密集型操作\"\"\"\n",
    "    print(f\"执行密集计算: {x} + {y}...\")\n",
    "    time.sleep(1.0)  # 模拟计算延迟\n",
    "    return x + y\n",
    "\n",
    "# 第一次计算\n",
    "start_time = time.time()\n",
    "result = compute_expensive(1000000, 2000000)\n",
    "print(f\"计算结果: {result}\")\n",
    "print(f\"计算耗时: {time.time() - start_time:.2f}秒\")\n",
    "\n",
    "# 再次计算相同参数，会命中缓存\n",
    "start_time = time.time()\n",
    "result = compute_expensive(1000000, 2000000)\n",
    "print(f\"缓存结果: {result}\")\n",
    "print(f\"缓存耗时: {time.time() - start_time:.2f}秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AsyncRAGGenerator 使用示例\n",
    "\n",
    "**场景说明：**\n",
    "- 场景1: 基本流式检索\n",
    "- 场景2: 处理多个文档\n",
    "- 场景3: 结合实际应用场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def example_async_rag_generator():\n",
    "    print(\"\\n=== AsyncRAGGenerator 使用示例 ===\")\n",
    "    \n",
    "    # 创建RAG生成器实例\n",
    "    generator = AsyncRAGGenerator(chunk_size=200)\n",
    "    \n",
    "    # 场景1: 基本流式检索\n",
    "    print(\"\\n1. 基本流式检索:\")\n",
    "    \n",
    "    # 准备测试文档\n",
    "    documents = [\n",
    "        \"Python是一种广泛使用的高级编程语言，由Guido van Rossum创建。Python的设计哲学强调代码的可读性和简洁的语法。\",\n",
    "        \"Python支持多种编程范式，包括面向对象、命令式、函数式和过程式编程。Python是一种解释型语言，意味着代码可以直接运行而不需要编译。\",\n",
    "        \"Python拥有丰富的标准库和第三方库，使其成为数据科学、人工智能、Web开发等领域的热门选择。\"\n",
    "    ]\n",
    "    \n",
    "    # 流式检索\n",
    "    print(\"查询: Python 编程语言\")\n",
    "    async for result in generator.stream_retrieval(\"Python 编程语言\", documents):\n",
    "        print(f\"  {result}\")\n",
    "    \n",
    "    # 场景2: 处理多个文档\n",
    "    print(\"\\n2. 处理多个文档:\")\n",
    "    \n",
    "    # 准备更多文档\n",
    "    more_documents = [\n",
    "        \"机器学习是人工智能的一个分支，专注于开发能够从数据中学习的算法。\",\n",
    "        \"深度学习是机器学习的一个子集，使用多层神经网络来模拟人脑的学习过程。\",\n",
    "        \"Python是机器学习和深度学习的首选语言，拥有丰富的库如TensorFlow、PyTorch和scikit-learn。\",\n",
    "        \"数据科学是一个跨学科领域，结合了统计学、计算机科学和领域知识来提取数据中的 insights。\"\n",
    "    ]\n",
    "    \n",
    "    # 流式检索\n",
    "    print(\"查询: Python 机器学习\")\n",
    "    async for result in generator.stream_retrieval(\"Python 机器学习\", more_documents):\n",
    "        print(f\"  {result}\")\n",
    "    \n",
    "    # 场景3: 结合实际应用场景\n",
    "    print(\"\\n3. 结合实际应用场景:\")\n",
    "    \n",
    "    async def rag_assistant(query: str, documents: list) -> str:\n",
    "        \"\"\"RAG助手：使用文档信息回答问题\"\"\"\n",
    "        print(f\"\\n用户问题: {query}\")\n",
    "        print(\"正在检索相关信息...\")\n",
    "        \n",
    "        # 收集相关信息\n",
    "        relevant_info = []\n",
    "        async for chunk in generator.stream_retrieval(query, documents):\n",
    "            relevant_info.append(chunk)\n",
    "        \n",
    "        # 生成回答\n",
    "        if relevant_info:\n",
    "            print(\"找到相关信息，生成回答...\")\n",
    "            return f\"根据检索到的信息，我可以回答您的问题: {query}\\\n",
    "\\\n",
    "相关信息:\\\n",
    "\" + \"\\n\".join(relevant_info)\n",
    "        else:\n",
    "            return f\"抱歉，我没有找到与 '{query}' 相关的信息。\"\n",
    "    \n",
    "    # 测试RAG助手\n",
    "    all_documents = documents + more_documents\n",
    "    response = await rag_assistant(\"Python在机器学习中的应用\", all_documents)\n",
    "    print(f\"\\n助手回答: {response}\")\n",
    "\n",
    "# 运行异步RAG生成器示例\n",
    "await example_async_rag_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ConcurrentRouter 使用示例\n",
    "\n",
    "**场景说明：**\n",
    "- 场景1: 基本路由\n",
    "- 场景2: 首选模型设置\n",
    "- 场景3: 自动降级\n",
    "- 场景4: 并发控制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def example_concurrent_router():\n",
    "    print(\"\\n=== ConcurrentRouter 使用示例 ===\")\n",
    "    \n",
    "    # 创建模型端点\n",
    "    endpoints = [\n",
    "        ModelEndpoint(name=\"gpt-4\", url=\"https://api.openai.com/v1/chat/completions\", max_concurrent=5),\n",
    "        ModelEndpoint(name=\"claude-3\", url=\"https://api.anthropic.com/v1/messages\", max_concurrent=3),\n",
    "        ModelEndpoint(name=\"gemini\", url=\"https://generativeai.googleapis.com/v1/models/gemini-pro:generateContent\", max_concurrent=4)\n",
    "    ]\n",
    "    \n",
    "    # 创建路由器\n",
    "    router = ConcurrentRouter(endpoints)\n",
    "    \n",
    "    # 场景1: 基本路由\n",
    "    print(\"\\n1. 基本路由:\")\n",
    "    result = await router.route_request(\"你好，请介绍一下自己\")\n",
    "    print(f\"使用模型: {result['model']}\")\n",
    "    print(f\"回复: {result['response']}\")\n",
    "    \n",
    "    # 场景2: 首选模型设置\n",
    "    print(\"\\n2. 首选模型设置:\")\n",
    "    result = await router.route_request(\"什么是量子计算?\", preferred_model=\"gpt-4\")\n",
    "    print(f\"使用模型: {result['model']}\")\n",
    "    print(f\"回复: {result['response']}\")\n",
    "    \n",
    "    # 场景3: 自动降级（模拟模型失败）\n",
    "    print(\"\\n3. 自动降级:\")\n",
    "    \n",
    "    # 保存原始调用方法\n",
    "    original_call_model = router._call_model\n",
    "    \n",
    "    async def mock_call_model(model_name, prompt):\n",
    "        \"\"\"模拟模型调用，使gpt-4失败\"\"\"\n",
    "        if model_name == \"gpt-4\":\n",
    "            print(f\"模拟模型 {model_name} 失败\")\n",
    "            raise Exception(\"Model service unavailable\")\n",
    "        return await original_call_model(model_name, prompt)\n",
    "    \n",
    "    # 替换调用方法\n",
    "    router._call_model = mock_call_model\n",
    "    \n",
    "    # 尝试使用首选模型gpt-4，应该会自动降级到其他模型\n",
    "    try:\n",
    "        result = await router.route_request(\"什么是人工智能?\", preferred_model=\"gpt-4\")\n",
    "        print(f\"使用模型: {result['model']}\")\n",
    "        print(f\"回复: {result['response']}\")\n",
    "        print(\"✓ 自动降级成功\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ 自动降级失败: {e}\")\n",
    "    \n",
    "    # 恢复原始调用方法\n",
    "    router._call_model = original_call_model\n",
    "    \n",
    "    # 场景4: 并发控制\n",
    "    print(\"\\n4. 并发控制:\")\n",
    "    \n",
    "    # 创建一个最大并发为1的模型端点\n",
    "    limited_endpoints = [\n",
    "        ModelEndpoint(name=\"limited-model\", url=\"http://example.com/model\", max_concurrent=1)\n",
    "    ]\n",
    "    limited_router = ConcurrentRouter(limited_endpoints)\n",
    "    \n",
    "    # 定义并发请求函数\n",
    "    async def make_request(i):\n",
    "        print(f\"发起请求 {i}...\")\n",
    "        start = time.time()\n",
    "        result = await limited_router.route_request(f\"测试请求 {i}\")\n",
    "        end = time.time()\n",
    "        print(f\"请求 {i} 完成，使用模型: {result['model']}，耗时: {end - start:.2f}秒\")\n",
    "        return result\n",
    "    \n",
    "    # 并发执行3个请求\n",
    "    print(\"执行3个并发请求（最大并发限制为1）:\")\n",
    "    start_time = time.time()\n",
    "    results = await asyncio.gather(\n",
    "        make_request(1),\n",
    "        make_request(2),\n",
    "        make_request(3)\n",
    "    )\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n所有请求完成，总耗时: {total_time:.2f}秒\")\n",
    "    print(\"注: 由于并发限制为1，请求会串行执行\")\n",
    "\n",
    "# 运行并发模型路由器示例\n",
    "await example_concurrent_router()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本 Notebook 演示了三个核心组件的使用方法：\n",
    "\n",
    "1. **DistributedCacheDecorator**：\n",
    "   - 缓存大模型API调用结果，提升性能\n",
    "   - 缓存数据库查询结果，减少重复查询\n",
    "   - 缓存计算密集型函数结果，优化计算效率\n",
    "\n",
    "2. **AsyncRAGGenerator**：\n",
    "   - 流式检索相关文档片段\n",
    "   - 处理多个文档的检索\n",
    "   - 结合实际应用场景，构建RAG问答助手\n",
    "\n",
    "3. **ConcurrentRouter**：\n",
    "   - 智能路由模型请求\n",
    "   - 设置首选模型\n",
    "   - 实现模型失败时的自动降级\n",
    "   - 控制并发请求数量\n",
    "\n",
    "这些组件可以根据实际需求进行定制和扩展，为构建企业级AI Agent系统提供了坚实的基础。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
